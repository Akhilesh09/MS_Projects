### YOUR CODE HERE
import tensorflow
import os, time
import numpy as np
from Network import MyNetwork
from ImageUtils import parse_record
import random
import math
from sklearn.metrics import plot_confusion_matrix
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler


"""This script defines the training, validation and testing process.
"""

class MyModel(object):

	def __init__(self, sess, configs):
		self.configs = configs
		self.sess = sess
		
		self.batch_size=configs["batch_size"]
		

	def model_setup(self,training):
		print('---Setup input interfaces...')

		print('---Setup the network...')
		self.net = MyNetwork(self.configs)

		self.model=self.net()

		if training:
			print('---Setup training components...')

			self.model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])

		else:

			print('---Loading saved model...')
			
			
			self.model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])

			self.model.load_weights("../saved_models/model.h5")

	def train(self, x_train, y_train,max_epoch):
		print('###Train###')

		self.model_setup(True)
		self.sess.run(tensorflow.compat.v1.global_variables_initializer())

		num_samples = x_train.shape[0]
		num_batches = int(num_samples / self.batch_size)

		x_batch_new=np.zeros((x_train.shape[0]+num_batches,3072))

		for j in range(num_samples):
			x_batch_new[j]=x_train[j]
		
		y_batch_new=np.zeros((y_train.shape[0]+num_batches,10))

		for j in range(y_train.shape[0]):
			y_batch_new[j]=y_train[j]

		## Mixup training
		## forming one new virtual example by linearly interpolating between 2 random examples to improve regularization num_batches times
		for i in range(num_batches):

			t=np.random.beta(0.1,0.1)

			rand_index1=random.randint(i*self.configs["batch_size"],(i+1)*self.configs["batch_size"]-1)
			while True:
				rand_index2=random.randint(i*self.configs["batch_size"],(i+1)*self.configs["batch_size"]-1)
				if(rand_index2!=rand_index1):
					break

			x_batch_new[x_train.shape[0]+i]=x_train[rand_index1]*t+x_train[rand_index2]*(1-t)

			y_batch_new[y_train.shape[0]+i]=y_train[rand_index1]*t+y_train[rand_index2]*(1-t)

		shuffle_index = np.random.permutation(num_samples+num_batches)
		curr_x_train = x_batch_new[shuffle_index]
		y_train = y_batch_new[shuffle_index]

		x_train = np.asarray([parse_record(i,True) for i in curr_x_train])
		
		print("Data parsed")

		lr_scheduler = LearningRateScheduler(lr_schedule)

		filepath = '../saved_models/model.h5'

		checkpoint = ModelCheckpoint(filepath=filepath,
							 monitor='accuracy',
							 verbose=1,
							 save_best_only=True)

		callbacks = [lr_scheduler,checkpoint]

		datagen = ImageDataGenerator(
		# set input mean to 0 over the dataset
		featurewise_center=True,
		# set each sample mean to 0
		samplewise_center=False,
		# divide inputs by std of dataset
		featurewise_std_normalization=True,
		# divide each input by its std
		samplewise_std_normalization=False,
		# apply ZCA whitening
		zca_whitening=False,
		# epsilon for ZCA whitening
		zca_epsilon=1e-06,
		# randomly rotate images in the range (deg 0 to 180)
		rotation_range=0,
		# randomly shift images horizontally
		width_shift_range=0.1,
		# randomly shift images vertically
		height_shift_range=0.1,
		# set range for random shear
		shear_range=0.,
		# set range for random zoom
		zoom_range=0.,
		# set range for random channel shifts
		channel_shift_range=0.,
		# set mode for filling points outside the input boundaries
		fill_mode='nearest',
		# value used for fill_mode = "constant"
		cval=0.,
		# randomly flip images
		horizontal_flip=True,
		# randomly flip images
		vertical_flip=False,
		# set rescaling factor (applied before any other transformation)
		rescale=None,
		# set function that will be applied on each input
		preprocessing_function=None,
		# image data format, either "channels_first" or "channels_last"
		data_format=None,
		# fraction of images reserved for validation (strictly between 0 and 1)
		validation_split=0.2)

		# Compute quantities required for featurewise normalization
		# (std, mean, and principal components if ZCA whitening is applied).
		datagen.fit(x_train)

		# Fit the model on the batches generated by datagen.flow().
		self.model.fit_generator(datagen.flow(x_train, y_train, batch_size=self.batch_size),
							epochs=max_epoch, verbose=1, workers=4,
							callbacks=callbacks)

	def evaluate(self, x, y):
		print('###Test or Validation###')

		self.model_setup(False)
		self.sess.run(tensorflow.compat.v1.global_variables_initializer())

		x = np.asarray([parse_record(i,False) for i in x])

		scores = self.model.evaluate(x, y, verbose=2)
		print('Test loss:', scores[0])
		print('Test accuracy:', scores[1])

		### Confusion Matrix

		# preds=self.model.predict(x)
		# pred_1d=np.argmax(preds,axis=-1)
		# y_1d=np.argmax(y,axis=-1)
		
		# conf_mat=tensorflow.math.confusion_matrix(pred_1d,y_1d)
		# with self.sess.as_default():
   		# 	print('Confusion Matrix: \n\n', tensorflow.Tensor.eval(conf_mat,feed_dict=None, session=None))

	def predict_prob(self, x):
		print('###Prediction###')

		self.model_setup(False)
		self.sess.run(tensorflow.compat.v1.global_variables_initializer())

		mean = np.mean(x)
		std = np.std(x)
		x = (x - mean) / std

		scores = np.round(self.model.predict(x),3)
		
		return scores


def lr_schedule(epoch):
	"""Learning Rate Schedule

	Learning rate is scheduled to be reduced using cosine decay.

	# Arguments
		epoch (int): The number of epochs

	# Returns
		lr (float32): learning rate
	"""
	lr_init = 1e-3

	lr = (1 + math.cos(math.pi * epoch / 200)) * lr_init / 2
	return lr